# Omniy ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ãƒªã‚¹ãƒˆã‚¢æ‰‹é †æ›¸

æœ€çµ‚æ›´æ–°æ—¥: 2025å¹´1æœˆ27æ—¥

## ğŸ¯ æ¦‚è¦

æœ¬æ–‡æ›¸ã¯ã€Omniy Instagramè‡ªå‹•æŠ•ç¨¿ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ç½å®³å¾©æ—§æ‰‹é †ã‚’å®šç¾©ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿æå¤±ã‚¼ãƒ­ã¨ã‚µãƒ¼ãƒ“ã‚¹ç¶™ç¶šæ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã¨è¿…é€Ÿãªå¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚

## ğŸ“Š ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

### RTO/RPO ç›®æ¨™

| ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— | RTO (å¾©æ—§ç›®æ¨™æ™‚é–“) | RPO (ãƒ‡ãƒ¼ã‚¿æå¤±è¨±å®¹æ™‚é–“) | ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é »åº¦ |
|--------------|-------------------|------------------------|------------------|
| **ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿** | 4æ™‚é–“ | 1æ™‚é–“ | 1æ™‚é–“æ¯ |
| **æŠ•ç¨¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„** | 2æ™‚é–“ | 30åˆ† | 30åˆ†æ¯ |
| **æŠ•ç¨¿å±¥æ­´** | 8æ™‚é–“ | 4æ™‚é–“ | 4æ™‚é–“æ¯ |
| **è¨­å®šãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿** | 1æ™‚é–“ | 15åˆ† | 15åˆ†æ¯ |
| **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰** | 30åˆ† | 0åˆ† | Git pushæ¯ |

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®3-2-1ãƒ«ãƒ¼ãƒ«

- **3ã¤ã®ã‚³ãƒ”ãƒ¼**: æœ¬ç•ªãƒ»ãƒ—ãƒ©ã‚¤ãƒãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- **2ã¤ã®ç•°ãªã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢**: Cloud Storageãƒ»BigQuery
- **1ã¤ã®ã‚ªãƒ•ã‚µã‚¤ãƒˆ**: ç•°ãªã‚‹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã§ã®ä¿ç®¡

---

## ğŸ—„ï¸ Firestore ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

### 1. è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š

#### 1.1 Cloud Schedulerè¨­å®š
```bash
# æ¯æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä½œæˆ
gcloud scheduler jobs create http firestore-backup-hourly \
  --location=asia-northeast1 \
  --schedule="0 * * * *" \
  --uri="https://us-central1-omniy-prod.cloudfunctions.net/firestoreBackup" \
  --http-method=POST \
  --headers="Content-Type=application/json" \
  --message-body='{"collections": ["users", "schedules", "posts", "igAccounts"]}'
```

#### 1.2 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–¢æ•°å®Ÿè£…
```javascript
// functions/src/backup/firestoreBackup.ts
import * as admin from 'firebase-admin';
import { Storage } from '@google-cloud/storage';

const storage = new Storage();
const projectId = 'omniy-prod';

export async function firestoreBackup(req: any, res: any) {
  try {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const outputUriPrefix = `gs://omniy-backups/firestore/${timestamp}`;
    
    // Firestore Exportå®Ÿè¡Œ
    const operation = await admin.firestore().export({
      databaseId: '(default)',
      outputUriPrefix: outputUriPrefix,
      collectionIds: req.body.collections || []
    });

    // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    await saveBackupMetadata({
      timestamp,
      type: 'firestore-full',
      status: 'running',
      operationName: operation.name,
      outputUri: outputUriPrefix,
      collections: req.body.collections
    });

    res.json({ 
      message: 'Backup started',
      operationName: operation.name,
      outputUri: outputUriPrefix
    });
  } catch (error) {
    console.error('Backup failed:', error);
    res.status(500).json({ error: 'Backup failed' });
  }
}

async function saveBackupMetadata(metadata: any) {
  await admin.firestore()
    .collection('_system')
    .doc('backups')
    .collection('history')
    .doc(metadata.timestamp)
    .set(metadata);
}
```

#### 1.3 å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…
```javascript
// å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–¢æ•°
export async function incrementalBackup(req: any, res: any) {
  try {
    const lastBackup = await getLastBackupTimestamp();
    const timestamp = new Date().toISOString();
    
    // å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    const changes = await extractChanges(lastBackup, timestamp);
    
    // Cloud Storage ã«ä¿å­˜
    const backupData = {
      timestamp,
      type: 'incremental',
      since: lastBackup,
      changes: changes
    };

    const bucket = storage.bucket('omniy-backups');
    const file = bucket.file(`incremental/${timestamp}.json`);
    
    await file.save(JSON.stringify(backupData, null, 2), {
      metadata: { contentType: 'application/json' }
    });

    res.json({ message: 'Incremental backup completed', timestamp });
  } catch (error) {
    console.error('Incremental backup failed:', error);
    res.status(500).json({ error: 'Incremental backup failed' });
  }
}

async function extractChanges(since: string, until: string) {
  const changes: any[] = [];
  const collections = ['users', 'schedules', 'posts', 'igAccounts'];

  for (const collection of collections) {
    const snapshot = await admin.firestore()
      .collection(collection)
      .where('updatedAt', '>', since)
      .where('updatedAt', '<=', until)
      .get();

    snapshot.docs.forEach(doc => {
      changes.push({
        collection,
        docId: doc.id,
        data: doc.data(),
        operation: doc.data().deletedAt ? 'delete' : 'upsert'
      });
    });
  }

  return changes;
}
```

### 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç›£è¦–

#### 2.1 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆåŠŸ/å¤±æ•—ã®ç›£è¦–
```javascript
// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç›£è¦–é–¢æ•°
export async function monitorBackups(req: any, res: any) {
  try {
    const last24Hours = new Date(Date.now() - 24 * 60 * 60 * 1000);
    
    const backups = await admin.firestore()
      .collection('_system')
      .doc('backups')
      .collection('history')
      .where('timestamp', '>', last24Hours.toISOString())
      .orderBy('timestamp', 'desc')
      .get();

    const backupStatus = {
      total: backups.size,
      successful: 0,
      failed: 0,
      running: 0,
      lastSuccessful: null,
      failures: []
    };

    backups.docs.forEach(doc => {
      const data = doc.data();
      switch (data.status) {
        case 'completed':
          backupStatus.successful++;
          if (!backupStatus.lastSuccessful || data.timestamp > backupStatus.lastSuccessful) {
            backupStatus.lastSuccessful = data.timestamp;
          }
          break;
        case 'failed':
          backupStatus.failed++;
          backupStatus.failures.push({
            timestamp: data.timestamp,
            error: data.error
          });
          break;
        case 'running':
          backupStatus.running++;
          break;
      }
    });

    // ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
    if (backupStatus.failed > 0 || backupStatus.successful === 0) {
      await sendBackupAlert(backupStatus);
    }

    res.json(backupStatus);
  } catch (error) {
    console.error('Backup monitoring failed:', error);
    res.status(500).json({ error: 'Monitoring failed' });
  }
}
```

---

## ğŸ”„ Cloud Storage ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

### 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š

#### 1.1 ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°æœ‰åŠ¹åŒ–
```bash
# ãƒã‚±ãƒƒãƒˆã§ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°æœ‰åŠ¹åŒ–
gsutil versioning set on gs://omniy-user-uploads
gsutil versioning set on gs://omniy-backups

# ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®š
cat > lifecycle.json << EOF
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
        "condition": {"age": 30}
      },
      {
        "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
        "condition": {"age": 90}
      },
      {
        "action": {"type": "Delete"},
        "condition": {"age": 365}
      }
    ]
  }
}
EOF

gsutil lifecycle set lifecycle.json gs://omniy-user-uploads
```

#### 1.2 ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
```bash
# ç•°ãªã‚‹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã¸ã®ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
gcloud storage buckets create gs://omniy-backups-us \
  --location=us-central1 \
  --storage-class=standard

# Transfer Service ã§ã®ã‚¯ãƒ­ã‚¹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
gcloud transfer jobs create \
  --source-gcs-bucket=omniy-backups \
  --sink-gcs-bucket=omniy-backups-us \
  --schedule-repeats-every=24h \
  --schedule-repeats-until=2026-01-01
```

### 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®åŒæœŸ

```javascript
// ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
export async function backupUserFiles(req: any, res: any) {
  try {
    const sourceBucket = storage.bucket('omniy-user-uploads');
    const backupBucket = storage.bucket('omniy-backups');
    
    const [files] = await sourceBucket.getFiles();
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    
    for (const file of files) {
      const backupPath = `files/${timestamp}/${file.name}`;
      await file.copy(backupBucket.file(backupPath));
    }

    res.json({ 
      message: 'File backup completed',
      fileCount: files.length,
      timestamp
    });
  } catch (error) {
    console.error('File backup failed:', error);
    res.status(500).json({ error: 'File backup failed' });
  }
}
```

---

## ğŸ“ BigQuery ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹

### 1. åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

#### 1.1 BigQuery Exportè¨­å®š
```javascript
// Firestore â†’ BigQuery Export
export async function exportToBigQuery(req: any, res: any) {
  try {
    const { BigQuery } = require('@google-cloud/bigquery');
    const bigquery = new BigQuery({ projectId: 'omniy-prod' });
    
    const dataset = bigquery.dataset('analytics');
    const timestamp = new Date().toISOString().split('T')[0].replace(/-/g, '');
    
    // ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æãƒ‡ãƒ¼ã‚¿
    await exportCollectionToBigQuery('users', `users_${timestamp}`, dataset);
    await exportCollectionToBigQuery('schedules', `schedules_${timestamp}`, dataset);
    await exportCollectionToBigQuery('posts', `posts_${timestamp}`, dataset);
    
    res.json({ message: 'BigQuery export completed', date: timestamp });
  } catch (error) {
    console.error('BigQuery export failed:', error);
    res.status(500).json({ error: 'BigQuery export failed' });
  }
}

async function exportCollectionToBigQuery(collectionName: string, tableName: string, dataset: any) {
  const docs = await admin.firestore().collection(collectionName).get();
  const rows = docs.docs.map(doc => ({
    id: doc.id,
    data: JSON.stringify(doc.data()),
    timestamp: new Date().toISOString()
  }));

  const table = dataset.table(tableName);
  await table.insert(rows);
}
```

#### 1.2 ãƒ‡ãƒ¼ã‚¿ä¿æŒãƒãƒªã‚·ãƒ¼
```sql
-- BigQuery ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰
CREATE TABLE `omniy-prod.analytics.users`
(
  id STRING,
  data JSON,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
)
PARTITION BY DATE(created_at)
CLUSTER BY id
OPTIONS(
  partition_expiration_days=2555,  -- 7å¹´é–“ä¿æŒ
  require_partition_filter=true
);
```

---

## ğŸ”§ å¾©æ—§æ‰‹é †

### 1. å®Œå…¨ç½å®³å¾©æ—§

#### 1.1 æ–°ç’°å¢ƒæ§‹ç¯‰
```bash
#!/bin/bash
# disaster-recovery.sh

set -e

PROJECT_ID="omniy-recovery"
BACKUP_BUCKET="omniy-backups"
LATEST_BACKUP=$(gsutil ls gs://${BACKUP_BUCKET}/firestore/ | tail -1)

echo "Starting disaster recovery process..."
echo "Latest backup: ${LATEST_BACKUP}"

# 1. Firebase ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
gcloud projects create ${PROJECT_ID}
gcloud config set project ${PROJECT_ID}

# 2. Firebase åˆæœŸåŒ–
firebase use ${PROJECT_ID}
firebase setup:emulators:firestore

# 3. Firestore å¾©å…ƒ
gcloud firestore operations list
gcloud firestore import ${LATEST_BACKUP} --database='(default)'

# 4. Cloud Functions ãƒ‡ãƒ—ãƒ­ã‚¤
cd functions
npm install
npm run build
firebase deploy --only functions

# 5. Hosting ãƒ‡ãƒ—ãƒ­ã‚¤  
cd ../frontend
npm install
npm run build
firebase deploy --only hosting

echo "Disaster recovery completed!"
```

#### 1.2 æ®µéšçš„å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹
```markdown
## Disaster Recovery Process

### Phase 1: Critical Systems (0-2 hours)
1. [ ] Firestore ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©å…ƒ
2. [ ] èªè¨¼ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§
3. [ ] åŸºæœ¬APIæ©Ÿèƒ½å¾©æ—§
4. [ ] ç·Šæ€¥å‘ŠçŸ¥ãƒšãƒ¼ã‚¸è¡¨ç¤º

### Phase 2: Core Features (2-8 hours)
1. [ ] InstagramæŠ•ç¨¿æ©Ÿèƒ½å¾©æ—§
2. [ ] ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å¾©æ—§
3. [ ] æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ å¾©æ—§
4. [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å¾©æ—§

### Phase 3: Advanced Features (8-24 hours)
1. [ ] å±¥æ­´ãƒ»åˆ†ææ©Ÿèƒ½å¾©æ—§
2. [ ] é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§
3. [ ] å…¨æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
4. [ ] ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆå¾©æ—§
```

### 2. éƒ¨åˆ†å¾©æ—§æ‰‹é †

#### 2.1 ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã®å¾©æ—§
```javascript
// ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å¾©æ—§
export async function restoreUserData(req: any, res: any) {
  try {
    const { userId, backupTimestamp } = req.body;
    
    // ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    const backupPath = `firestore/${backupTimestamp}`;
    const userData = await getBackupData(backupPath, 'users', userId);
    
    if (!userData) {
      return res.status(404).json({ error: 'Backup data not found' });
    }

    // ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    const currentData = await admin.firestore()
      .collection('users')
      .doc(userId)
      .get();

    if (currentData.exists) {
      await saveRestorePoint(userId, currentData.data());
    }

    // ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
    await admin.firestore()
      .collection('users')
      .doc(userId)
      .set(userData);

    res.json({ 
      message: 'User data restored successfully',
      userId,
      backupTimestamp
    });
  } catch (error) {
    console.error('User data restore failed:', error);
    res.status(500).json({ error: 'Restore failed' });
  }
}

async function saveRestorePoint(userId: string, data: any) {
  const timestamp = new Date().toISOString();
  await admin.firestore()
    .collection('_system')
    .doc('restores')
    .collection('points')
    .doc(`${userId}_${timestamp}`)
    .set({
      userId,
      data,
      timestamp,
      type: 'pre-restore-backup'
    });
}
```

#### 2.2 ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
```javascript
// å¾©æ—§å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
export async function validateDataIntegrity(req: any, res: any) {
  try {
    const issues: any[] = [];
    
    // ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†” ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ•´åˆæ€§
    const users = await admin.firestore().collection('users').get();
    for (const user of users.docs) {
      const schedules = await admin.firestore()
        .collection('schedules')
        .where('userId', '==', user.id)
        .get();
      
      const userScheduleCount = user.data().scheduleCount || 0;
      if (schedules.size !== userScheduleCount) {
        issues.push({
          type: 'schedule_count_mismatch',
          userId: user.id,
          expected: userScheduleCount,
          actual: schedules.size
        });
      }
    }

    // IGã‚¢ã‚«ã‚¦ãƒ³ãƒˆ â†” ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ•´åˆæ€§
    const igAccounts = await admin.firestore().collection('igAccounts').get();
    for (const account of igAccounts.docs) {
      const schedules = await admin.firestore()
        .collection('schedules')
        .where('igAccountId', '==', account.id)
        .get();
        
      for (const schedule of schedules.docs) {
        if (schedule.data().userId !== account.data().userId) {
          issues.push({
            type: 'account_user_mismatch',
            scheduleId: schedule.id,
            accountUserId: account.data().userId,
            scheduleUserId: schedule.data().userId
          });
        }
      }
    }

    res.json({
      valid: issues.length === 0,
      issues,
      checkedAt: new Date().toISOString()
    });
  } catch (error) {
    console.error('Data integrity check failed:', error);
    res.status(500).json({ error: 'Integrity check failed' });
  }
}
```

---

## ğŸ§ª ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ

### 1. å®šæœŸå¾©æ—§ãƒ†ã‚¹ãƒˆ

#### 1.1 æœˆæ¬¡å¾©æ—§ãƒ†ã‚¹ãƒˆæ‰‹é †
```bash
#!/bin/bash
# monthly-restore-test.sh

RECOVERY_PROJECT="omniy-test-recovery"
TEST_DATE=$(date +%Y%m%d)
LATEST_BACKUP=$(gsutil ls gs://omniy-backups/firestore/ | tail -1)

echo "Starting monthly restore test: ${TEST_DATE}"

# ãƒ†ã‚¹ãƒˆç’°å¢ƒä½œæˆ
gcloud projects create ${RECOVERY_PROJECT}-${TEST_DATE}
gcloud config set project ${RECOVERY_PROJECT}-${TEST_DATE}

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒãƒ†ã‚¹ãƒˆ
gcloud firestore import ${LATEST_BACKUP} --database='(default)'

# åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
npm run test:restore -- --project=${RECOVERY_PROJECT}-${TEST_DATE}

# æ€§èƒ½ãƒ†ã‚¹ãƒˆ
npm run test:performance -- --project=${RECOVERY_PROJECT}-${TEST_DATE}

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
generate_restore_test_report ${TEST_DATE}

# ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
gcloud projects delete ${RECOVERY_PROJECT}-${TEST_DATE} --quiet

echo "Monthly restore test completed: ${TEST_DATE}"
```

#### 1.2 è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```javascript
// restore-test.js
const admin = require('firebase-admin');
const { expect } = require('chai');

describe('Restore Validation Tests', () => {
  before(async () => {
    admin.initializeApp({
      projectId: process.env.TEST_PROJECT_ID
    });
  });

  it('should have all required collections', async () => {
    const collections = ['users', 'schedules', 'posts', 'igAccounts'];
    
    for (const collection of collections) {
      const snapshot = await admin.firestore().collection(collection).limit(1).get();
      expect(snapshot.empty).to.be.false;
    }
  });

  it('should maintain data relationships', async () => {
    const users = await admin.firestore().collection('users').limit(10).get();
    
    for (const user of users.docs) {
      const schedules = await admin.firestore()
        .collection('schedules')
        .where('userId', '==', user.id)
        .get();
      
      for (const schedule of schedules.docs) {
        expect(schedule.data().userId).to.equal(user.id);
      }
    }
  });

  it('should have recent data', async () => {
    const yesterday = new Date(Date.now() - 24 * 60 * 60 * 1000);
    
    const recentSchedules = await admin.firestore()
      .collection('schedules')
      .where('createdAt', '>', yesterday)
      .get();
    
    expect(recentSchedules.size).to.be.greaterThan(0);
  });
});
```

### 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å“è³ªãƒã‚§ãƒƒã‚¯

```javascript
// ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å“è³ªæ¤œè¨¼
export async function validateBackupQuality(req: any, res: any) {
  try {
    const { backupTimestamp } = req.body;
    const issues: any[] = [];
    
    // 1. ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
    const backupMetadata = await getBackupMetadata(backupTimestamp);
    const expectedCollections = ['users', 'schedules', 'posts', 'igAccounts'];
    
    for (const collection of expectedCollections) {
      const count = await getBackupCollectionCount(backupTimestamp, collection);
      if (count === 0) {
        issues.push({
          type: 'empty_collection',
          collection,
          count
        });
      }
    }

    // 2. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    const dataConsistency = await checkBackupConsistency(backupTimestamp);
    issues.push(...dataConsistency);

    // 3. ã‚µã‚¤ã‚ºå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    const backupSize = await getBackupSize(backupTimestamp);
    const previousBackupSize = await getPreviousBackupSize();
    
    if (backupSize < previousBackupSize * 0.8) {
      issues.push({
        type: 'unexpected_size_decrease',
        currentSize: backupSize,
        previousSize: previousBackupSize,
        ratio: backupSize / previousBackupSize
      });
    }

    res.json({
      valid: issues.length === 0,
      issues,
      backupTimestamp,
      validatedAt: new Date().toISOString()
    });
  } catch (error) {
    console.error('Backup validation failed:', error);
    res.status(500).json({ error: 'Validation failed' });
  }
}
```

---

## ğŸ“‹ é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### æ—¥æ¬¡ç¢ºèªäº‹é …
```markdown
## Daily Backup Checklist

### Morning (9:00 AM)
- [ ] å‰æ—¥ã®è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆåŠŸç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
- [ ] ç•°å¸¸ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ç¢ºèª

### Evening (6:00 PM)
- [ ] å½“æ—¥ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ³ç¢ºèª
- [ ] ç¿Œæ—¥ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹äºˆå®šç¢ºèª
- [ ] é€±æ¬¡/æœˆæ¬¡ãƒ†ã‚¹ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèª
```

### é€±æ¬¡ç¢ºèªäº‹é …
```markdown
## Weekly Backup Review

### Every Monday
- [ ] é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å“è³ªãƒ¬ãƒãƒ¼ãƒˆç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒãƒãƒªã‚·ãƒ¼éµå®ˆç¢ºèª
- [ ] å¾©æ—§ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚³ã‚¹ãƒˆæœ€é©åŒ–ãƒã‚§ãƒƒã‚¯

### Every Friday  
- [ ] æ¥é€±ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèª
- [ ] é‡è¦å¤‰æ›´äºˆå®šã¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨ˆç”»èª¿æ•´
- [ ] æœˆæ¬¡å¾©æ—§ãƒ†ã‚¹ãƒˆæº–å‚™ï¼ˆè©²å½“é€±ã®ã¿ï¼‰
```

### æœˆæ¬¡ç¢ºèªäº‹é …
```markdown
## Monthly Backup Audit

### First Monday of Month
- [ ] å®Œå…¨ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥è¦‹ç›´ã—
- [ ] RTO/RPOç›®æ¨™é”æˆçŠ¶æ³ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚³ã‚¹ãƒˆåˆ†æ

### Third Monday of Month  
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°
- [ ] ä¿æŒãƒãƒªã‚·ãƒ¼æœ€é©åŒ–æ¤œè¨
- [ ] ç½å®³å¾©æ—§æ‰‹é †æ›¸æ›´æ–°
- [ ] ãƒãƒ¼ãƒ ç ”ä¿®ãƒ»ãƒ‰ãƒªãƒ«å®Ÿæ–½
```

---

## ğŸ”— é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [éšœå®³å¯¾å¿œæ‰‹é †æ›¸](./INCIDENT_RESPONSE.md)
- [ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š](./MONITORING_SETUP.md)
- [é‹ç”¨æ‰‹é †æ›¸](./OPERATIONS.md)
- [æŠ€è¡“ä»•æ§˜æ›¸](./TECHNICAL_SPECIFICATIONS.md)

---

**ä½œæˆè€…**: ã‚¤ãƒ³ãƒ•ãƒ©ãƒãƒ¼ãƒ   
**æ‰¿èªè€…**: CTO  
**æ¬¡å›è¦‹ç›´ã—**: 2025å¹´4æœˆ27æ—¥